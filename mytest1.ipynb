{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('bin/')\n",
    "\n",
    "from lsnn.toolbox.tensorflow_einsums.einsum_re_written import einsum_bij_jk_to_bik\n",
    "\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from lsnn.toolbox.file_saver_dumper_no_h5py import save_file, get_storage_path_reference\n",
    "from tutorial_sequential_mnist_plot import update_mnist_plot\n",
    "\n",
    "from lsnn.spiking_models import tf_cell_to_savable_dict, exp_convolve, ALIF, LIF, MyLIF, NoReset, TheirReset\n",
    "from lsnn.toolbox.rewiring_tools import weight_sampler, rewiring_optimizer_wrapper\n",
    "from lsnn.toolbox.tensorflow_utils import tf_downsample\n",
    "import json\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "__file__ = 'test'\n",
    "#run_options = tf.RunOptions(report_tensor_allocations_upon_oom = True)\n",
    "run_options = tf.RunOptions(report_tensor_allocations_upon_oom = False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_regular\": 120,\n",
      "    \"n_batch_train\": 256,\n",
      "    \"tau_v\": 20.0,\n",
      "    \"l1\": 0.01,\n",
      "    \"n_batch_validation\": 256,\n",
      "    \"neuron_sign\": true,\n",
      "    \"thr\": 0.01,\n",
      "    \"ext_time\": 1,\n",
      "    \"downsampled\": false,\n",
      "    \"tau_a\": 700.0,\n",
      "    \"lr_decay_every\": 2500,\n",
      "    \"n_iter\": 36000,\n",
      "    \"proportion_excitatory\": 0.75,\n",
      "    \"reg\": 0.1,\n",
      "    \"lr_decay\": 0.8,\n",
      "    \"crs_thr\": true,\n",
      "    \"beta\": 1.8,\n",
      "    \"model\": \"LSNN\",\n",
      "    \"rewiring_temperature\": 0.0,\n",
      "    \"comment\": \"\",\n",
      "    \"resume\": \"\",\n",
      "    \"save_data\": false,\n",
      "    \"print_every\": 400,\n",
      "    \"n_delay\": 10,\n",
      "    \"f\": \"/home/developer/.local/share/jupyter/runtime/kernel-b9f77b2d-b254-4b99-ae7a-1c00f9324444.json\",\n",
      "    \"n_ref\": 5,\n",
      "    \"learning_rate\": 0.01,\n",
      "    \"dampening_factor\": 0.3,\n",
      "    \"reg_rate\": 10,\n",
      "    \"verbose\": true,\n",
      "    \"rewiring_connectivity\": 0.12,\n",
      "    \"n_adaptive\": 100,\n",
      "    \"n_in\": 80,\n",
      "    \"interactive_plot\": false\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "##\n",
    "tf.app.flags.DEFINE_string('comment', '', 'comment to retrieve the stored results')\n",
    "tf.app.flags.DEFINE_string('resume', '', 'path to the checkpoint of the form \"results/script_name/2018_.../session\"')\n",
    "tf.app.flags.DEFINE_string('model', 'LSNN', 'spiking network model to use: LSNN or LIF')\n",
    "tf.app.flags.DEFINE_bool('save_data', False, 'whether to save simulation data in result folder')\n",
    "tf.app.flags.DEFINE_bool('downsampled', False, 'whether to use the smaller downsampled mnist dataset of not')\n",
    "##\n",
    "tf.app.flags.DEFINE_integer('n_batch_train', 256, 'size of the training minibatch')\n",
    "tf.app.flags.DEFINE_integer('n_batch_validation', 256, 'size of the validation minibatch')\n",
    "tf.app.flags.DEFINE_integer('n_in', 80, 'number of input units')\n",
    "tf.app.flags.DEFINE_integer('n_regular', 120, 'number of regular spiking units in the recurrent layer')\n",
    "tf.app.flags.DEFINE_integer('n_adaptive', 100, 'number of adaptive spiking units in the recurrent layer')\n",
    "tf.app.flags.DEFINE_integer('reg_rate', 10, 'target firing rate for regularization')\n",
    "tf.app.flags.DEFINE_integer('n_iter', 36000, 'number of training iterations')\n",
    "tf.app.flags.DEFINE_integer('n_delay', 10, 'maximum synaptic delay')\n",
    "tf.app.flags.DEFINE_integer('n_ref', 5, 'number of refractory steps')\n",
    "tf.app.flags.DEFINE_integer('lr_decay_every', 2500, 'decay learning rate every lr_decay_every steps')\n",
    "tf.app.flags.DEFINE_integer('print_every', 400, 'frequency of validation')\n",
    "tf.app.flags.DEFINE_integer('ext_time', 1, 'repeat factor to extend time of mnist task')\n",
    "##\n",
    "tf.app.flags.DEFINE_float('beta', 1.8, 'Scaling constant of the adaptive threshold')\n",
    "# to solve a task successfully we usually set tau_a to be close to the expected delay / memory length needed\n",
    "tf.app.flags.DEFINE_float('tau_a', 700, 'Adaptation time constant')\n",
    "tf.app.flags.DEFINE_float('tau_v', 20, 'Membrane time constant of output readouts')\n",
    "tf.app.flags.DEFINE_float('thr', 0.01, 'Baseline threshold voltage')\n",
    "tf.app.flags.DEFINE_float('learning_rate', 1e-2, 'Base learning rate')\n",
    "tf.app.flags.DEFINE_float('lr_decay', 0.8, 'Decaying factor')\n",
    "tf.app.flags.DEFINE_float('reg', 1e-1, 'regularization coefficient to target a specific firing rate')\n",
    "tf.app.flags.DEFINE_float('rewiring_temperature', 0., 'regularization coefficient')\n",
    "tf.app.flags.DEFINE_float('proportion_excitatory', 0.75, 'proportion of excitatory neurons')\n",
    "##\n",
    "tf.app.flags.DEFINE_bool('interactive_plot', False, 'Perform plotting')\n",
    "tf.app.flags.DEFINE_bool('verbose', True, 'Print many info during training')\n",
    "tf.app.flags.DEFINE_bool('neuron_sign', True,\n",
    "                         \"If rewiring is active, this will fix the sign of neurons (Dale's law)\")\n",
    "tf.app.flags.DEFINE_bool('crs_thr', True, 'Encode pixels to spikes with threshold crossing method')\n",
    "# With simple grid search we found that setting rewiring to 12% yields optimal results\n",
    "tf.app.flags.DEFINE_float('rewiring_connectivity', 0.12, 'max connectivity limit in the network (-1 turns off DEEP R)')\n",
    "tf.app.flags.DEFINE_float('l1', 1e-2, 'l1 regularization used in rewiring (irrelevant without rewiring)')\n",
    "tf.app.flags.DEFINE_float('dampening_factor', 0.3, 'Parameter necessary to approximate the spike derivative')\n",
    "# Analog values are fed to only single neuron\n",
    "\n",
    "if not FLAGS.crs_thr:\n",
    "    FLAGS.n_in = 1\n",
    "\n",
    "assert FLAGS.model in ['LSNN', 'LIF'], \"Model must be LSNN or LIF\"\n",
    "assert not (FLAGS.model == 'LIF' and FLAGS.n_adaptive > 0), \"LIF network can not contain adaptive neurons!\"\n",
    "\n",
    "# Define the flag object as dictionnary for saving purposes\n",
    "_, storage_path, flag_dict = get_storage_path_reference(__file__, FLAGS, './results/', flags=False,\n",
    "                                                        comment=len(FLAGS.comment) > 0)\n",
    "if FLAGS.save_data:\n",
    "    os.makedirs(storage_path, exist_ok=True)\n",
    "    save_file(flag_dict, storage_path, 'flag', 'json')\n",
    "    print('saving data to: ' + storage_path)\n",
    "print(json.dumps(flag_dict, indent=4))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "FLAGS.n_batch_train = 128#64\n",
    "FLAGS.n_batch_validation = 128\n",
    "FLAGS.rewiring_connectivity = -1\n",
    "FLAGS.neuron_sign = False\n",
    "FLAGS.reg = 0\n",
    "FLAGS.thr = 1\n",
    "FLAGS.n_delay = 1\n",
    "FLAGS.n_ref = 0\n",
    "#FLAGS.n_adaptive = 0\n",
    "#FLAGS.n_regular = 0\n",
    "#FLAGS.model = 'LSNN'\n",
    "#FLAGS.model = 'TheirReset'\n",
    "#FLAGS.learning_rate = 3e-3\n",
    "#FLAGS.interactive_plot = True\n",
    "#FLAGS.n_iter = 1001\n",
    "#FLAGS.print_every = 200\n",
    "\n",
    "\n",
    "#with delay: .75 after 6k .6 after 1.6k\n",
    "#without delay: .66 after 6k .54 after 1.6k, randomly .75 after 7k"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-bdc99b20b19f>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# Fix the random seed if given as an argument\n",
    "dt = 1.  # Time step is by default 1 ms\n",
    "n_output_symbols = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# Sign of the neurons\n",
    "if 0 < FLAGS.rewiring_connectivity and FLAGS.neuron_sign:\n",
    "    n_excitatory_in = int(FLAGS.proportion_excitatory * FLAGS.n_in) + 1\n",
    "    n_inhibitory_in = FLAGS.n_in - n_excitatory_in\n",
    "    in_neuron_sign = np.concatenate([-np.ones(n_inhibitory_in), np.ones(n_excitatory_in)])\n",
    "    np.random.shuffle(in_neuron_sign)\n",
    "\n",
    "    n_excitatory = int(FLAGS.proportion_excitatory * (FLAGS.n_regular + FLAGS.n_adaptive)) + 1\n",
    "    n_inhibitory = FLAGS.n_regular + FLAGS.n_adaptive - n_excitatory\n",
    "    rec_neuron_sign = np.concatenate([-np.ones(n_inhibitory), np.ones(n_excitatory)])\n",
    "else:\n",
    "    if not (FLAGS.neuron_sign == False): print(\n",
    "        'WARNING: Neuron sign is set to None without rewiring but sign is requested')\n",
    "    in_neuron_sign = None\n",
    "    rec_neuron_sign = None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "#TODO: change back MyLIF\n",
    "# Define the network\n",
    "if FLAGS.model == 'LSNN':\n",
    "    # We set beta == 0 to some of the neurons. Those neurons then behave like LIF neurons (without adaptation).\n",
    "    # And this is how we achieve a mixture of LIF and ALIF neurons in the LSNN model.\n",
    "    beta = np.concatenate([np.zeros(FLAGS.n_regular), np.ones(FLAGS.n_adaptive) * FLAGS.beta])\n",
    "    cell = ALIF(n_in=FLAGS.n_in, n_rec=FLAGS.n_regular + FLAGS.n_adaptive, tau=FLAGS.tau_v, n_delay=FLAGS.n_delay,\n",
    "                n_refractory=FLAGS.n_ref, dt=dt, tau_adaptation=FLAGS.tau_a, beta=beta, thr=FLAGS.thr,\n",
    "                rewiring_connectivity=FLAGS.rewiring_connectivity,\n",
    "                in_neuron_sign=in_neuron_sign, rec_neuron_sign=rec_neuron_sign,\n",
    "                dampening_factor=FLAGS.dampening_factor,\n",
    "                )\n",
    "elif FLAGS.model == 'LIF':\n",
    "    cell = LIF(n_in=FLAGS.n_in, n_rec=FLAGS.n_regular + FLAGS.n_adaptive, tau=FLAGS.tau_v, n_delay=FLAGS.n_delay,\n",
    "               n_refractory=FLAGS.n_ref, dt=dt, thr=FLAGS.thr,\n",
    "               rewiring_connectivity=FLAGS.rewiring_connectivity,\n",
    "               in_neuron_sign=in_neuron_sign, rec_neuron_sign=rec_neuron_sign,\n",
    "               dampening_factor=FLAGS.dampening_factor\n",
    "               )\n",
    "elif FLAGS.model == 'NoReset':\n",
    "    reset = np.concatenate([np.zeros(FLAGS.n_adaptive), np.ones(FLAGS.n_regular)])\n",
    "    cell = NoReset(n_in=FLAGS.n_in, n_rec=FLAGS.n_regular + FLAGS.n_adaptive, tau=FLAGS.tau_v, n_delay=FLAGS.n_delay,\n",
    "               reset=reset, dt=dt, thr=FLAGS.thr,\n",
    "               rewiring_connectivity=FLAGS.rewiring_connectivity,\n",
    "               in_neuron_sign=in_neuron_sign, rec_neuron_sign=rec_neuron_sign,\n",
    "               dampening_factor=FLAGS.dampening_factor\n",
    "               )\n",
    "elif FLAGS.model == 'TheirReset':\n",
    "    reset = np.concatenate([np.zeros(FLAGS.n_adaptive), np.ones(FLAGS.n_regular)])\n",
    "    cell = TheirReset(n_in=FLAGS.n_in, n_rec=FLAGS.n_regular + FLAGS.n_adaptive, tau=FLAGS.tau_v, n_delay=FLAGS.n_delay,\n",
    "               n_refractory=FLAGS.n_ref, dt=dt, thr=FLAGS.thr,\n",
    "               rewiring_connectivity=FLAGS.rewiring_connectivity,\n",
    "               in_neuron_sign=in_neuron_sign, rec_neuron_sign=rec_neuron_sign,\n",
    "               dampening_factor=FLAGS.dampening_factor, reset=reset,\n",
    "               )\n",
    "else:\n",
    "    raise NotImplementedError(\"Unknown model: \" + FLAGS.model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Generate input\n",
    "input_pixels = tf.placeholder(dtype=tf.float32, shape=(None, None, FLAGS.n_in),\n",
    "                              name='InputSpikes')  # MAIN input spike placeholder\n",
    "\n",
    "targets = tf.placeholder(dtype=tf.int64, shape=(None,),\n",
    "                         name='Targets')  # Lists of target characters of the recall task\n",
    "\n",
    "\n",
    "def find_onset_offset(y, threshold):\n",
    "    \"\"\"\n",
    "    Given the input signal `y` with samples,\n",
    "    find the indices where `y` increases and descreases through the value `threshold`.\n",
    "    Return stacked binary arrays of shape `y` indicating onset and offset threshold crossings.\n",
    "    `y` must be 1-D numpy arrays.\n",
    "    \"\"\"\n",
    "    if threshold == 1:\n",
    "        equal = y == threshold\n",
    "        transition_touch = np.where(equal)[0]\n",
    "        touch_spikes = np.zeros_like(y)\n",
    "        touch_spikes[transition_touch] = 1\n",
    "        return np.expand_dims(touch_spikes, axis=0)\n",
    "    else:\n",
    "        # Find where y crosses the threshold (increasing).\n",
    "        lower = y < threshold\n",
    "        higher = y >= threshold\n",
    "        transition_onset = np.where(lower[:-1] & higher[1:])[0]\n",
    "        transition_offset = np.where(higher[:-1] & lower[1:])[0]\n",
    "        onset_spikes = np.zeros_like(y)\n",
    "        offset_spikes = np.zeros_like(y)\n",
    "        onset_spikes[transition_onset] = 1\n",
    "        offset_spikes[transition_offset] = 1\n",
    "\n",
    "        return np.stack((onset_spikes, offset_spikes))\n",
    "\n",
    "\n",
    "def get_data_dict(batch_size, type='train'):\n",
    "    \"\"\"\n",
    "    Generate the dictionary to be fed when running a tensorflow op.\n",
    "    \"\"\"\n",
    "    if type == 'test':\n",
    "        input_px, target_oh = mnist.test.next_batch(batch_size, shuffle=False)\n",
    "    elif type == 'validation':\n",
    "        input_px, target_oh = mnist.validation.next_batch(batch_size)\n",
    "    elif type == 'train':\n",
    "        input_px, target_oh = mnist.train.next_batch(batch_size)\n",
    "    else:\n",
    "        raise ValueError(\"Wrong data group: \" + str(type))\n",
    "\n",
    "    target_num = np.argmax(target_oh, axis=1)\n",
    "\n",
    "    if FLAGS.ext_time > 1:\n",
    "        input_px = np.repeat(input_px, FLAGS.ext_time, axis=1)\n",
    "\n",
    "    if FLAGS.crs_thr:\n",
    "        # GENERATE THRESHOLD CROSSING SPIKES\n",
    "        thrs = np.linspace(0, 1, FLAGS.n_in // 2)  # number of input neurons determins the resolution\n",
    "        spike_stack = []\n",
    "        for img in input_px:  # shape img = (784)\n",
    "            Sspikes = None\n",
    "            for thr in thrs:\n",
    "                if Sspikes is not None:\n",
    "                    Sspikes = np.concatenate((Sspikes, find_onset_offset(img, thr)))\n",
    "                else:\n",
    "                    Sspikes = find_onset_offset(img, thr)\n",
    "            Sspikes = np.array(Sspikes)  # shape Sspikes = (31, 784)\n",
    "            Sspikes = np.swapaxes(Sspikes, 0, 1)\n",
    "            spike_stack.append(Sspikes)\n",
    "        spike_stack = np.array(spike_stack)\n",
    "        # add output cue neuron, and expand time for two image rows (2*28)\n",
    "        out_cue_duration = 2 * 28 * FLAGS.ext_time\n",
    "        spike_stack = np.lib.pad(spike_stack, ((0, 0), (0, out_cue_duration), (0, 1)), 'constant')\n",
    "        # output cue neuron fires constantly for these additional recall steps\n",
    "        spike_stack[:, -out_cue_duration:, -1] = 1\n",
    "    else:\n",
    "        spike_stack = input_px\n",
    "        spike_stack = np.expand_dims(spike_stack, axis=2)\n",
    "        # # match input dimensionality (add inactive output cue neuron)\n",
    "        # spike_stack = np.lib.pad(spike_stack, ((0, 0), (0, 0), (0, 1)), 'constant')\n",
    "\n",
    "    # transform target one hot from batch x classes to batch x time x classes\n",
    "    data_dict = {input_pixels: spike_stack, targets: target_num}\n",
    "    return data_dict, input_px\n",
    "\n",
    "\n",
    "if not FLAGS.crs_thr and FLAGS.downsampled:\n",
    "    inputs = tf.reshape(input_pixels,[-1,28,28,1])\n",
    "    inputs = tf.layers.average_pooling2d(inputs,pool_size=2,strides=2,name='DownSampleWithPool',padding='same')\n",
    "    inputs = tf.reshape(inputs,[-1,14 * 14,1])\n",
    "else:\n",
    "    inputs = input_pixels"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "if FLAGS.model == 'LSNN':\n",
    "    z, v, b = outputs\n",
    "else:\n",
    "    z = outputs\n",
    "z_regular = z[:, :, :FLAGS.n_regular]\n",
    "z_adaptive = z[:, :, FLAGS.n_regular:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "#TODO: post-synaptic potential? i had this off by = 0\n",
    "with tf.name_scope('ClassificationLoss'):\n",
    "    psp_decay = np.exp(-dt / FLAGS.tau_v)  # output layer psp decay, chose value between 15 and 30ms as for tau_v\n",
    "    psp = exp_convolve(z, decay=psp_decay)\n",
    "    n_neurons = z.get_shape()[2]\n",
    "\n",
    "    # Define the readout weights\n",
    "    if 0 < FLAGS.rewiring_connectivity:\n",
    "        w_out, w_out_sign, w_out_var, _ = weight_sampler(FLAGS.n_regular + FLAGS.n_adaptive, n_output_symbols,\n",
    "                                                         FLAGS.rewiring_connectivity,\n",
    "                                                         neuron_sign=rec_neuron_sign)\n",
    "    else:\n",
    "        w_out = tf.get_variable(name='out_weight', shape=[n_neurons, n_output_symbols])\n",
    "    b_out = tf.get_variable(name='out_bias', shape=[n_output_symbols], initializer=tf.zeros_initializer())\n",
    "\n",
    "    # Define the loss function\n",
    "    out = einsum_bij_jk_to_bik(psp, w_out) + b_out\n",
    "\n",
    "    if FLAGS.crs_thr:\n",
    "        outt = tf_downsample(out, new_size=(28+2) * FLAGS.ext_time, axis=1)  # 32 x 30 x 10\n",
    "        Y_predict = outt[:, -1, :]  # shape batch x classes == n_batch x 10\n",
    "    else:\n",
    "        Y_predict = out[:, -1, :]  # shape batch x classes == n_batch x 10\n",
    "\n",
    "    loss_recall = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=targets, logits=Y_predict))\n",
    "\n",
    "    with tf.name_scope('PlotNodes'):\n",
    "        out_plot = tf.nn.softmax(out)\n",
    "\n",
    "    # Define the accuracy\n",
    "    Y_predict_num = tf.argmax(Y_predict, axis=1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(targets, Y_predict_num), dtype=tf.float32))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Target regularization\n",
    "with tf.name_scope('RegularizationLoss'):\n",
    "    # Firing rate regularization\n",
    "    av = tf.reduce_mean(z, axis=(0, 1)) / dt\n",
    "    regularization_f0 = FLAGS.reg_rate / 1000\n",
    "    loss_regularization = tf.reduce_sum(tf.square(av - regularization_f0)) * FLAGS.reg\n",
    "\n",
    "# Aggregate the losses\n",
    "with tf.name_scope('OptimizationScheme'):\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False)\n",
    "    learning_rate = tf.Variable(FLAGS.learning_rate, dtype=tf.float32, trainable=False)\n",
    "    decay_learning_rate_op = tf.assign(learning_rate, learning_rate * FLAGS.lr_decay)  # Op to decay learning rate\n",
    "\n",
    "    loss = loss_regularization + loss_recall\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    if 0 < FLAGS.rewiring_connectivity:\n",
    "\n",
    "        train_step = rewiring_optimizer_wrapper(optimizer, loss, learning_rate, FLAGS.l1, FLAGS.rewiring_temperature,\n",
    "                                                FLAGS.rewiring_connectivity,\n",
    "                                                global_step=global_step,\n",
    "                                                var_list=tf.trainable_variables())\n",
    "    else:\n",
    "        train_step = optimizer.minimize(loss=loss, global_step=global_step)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Real-time plotting\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if FLAGS.resume:\n",
    "    saver.restore(sess, FLAGS.resume)\n",
    "    print(\"Model restored.\")\n",
    "\n",
    "# Open an interactive matplotlib window to plot in real time\n",
    "if FLAGS.interactive_plot:\n",
    "    plt.ion()\n",
    "    fig, ax_list = plt.subplots(5, figsize=(6, 7.5), gridspec_kw={'wspace':0, 'hspace':0.2})\n",
    "\n",
    "# Store some results across iterations\n",
    "test_loss_list = []\n",
    "test_loss_with_reg_list = []\n",
    "test_error_list = []\n",
    "tau_delay_list = []\n",
    "training_time_list = []\n",
    "time_to_ref_list = []\n",
    "\n",
    "# Dictionaries of tensorflow ops to be evaluated simultaneously by a session\n",
    "results_tensors = {'loss': loss,\n",
    "                   'loss_reg': loss_regularization,\n",
    "                   'loss_recall': loss_recall,\n",
    "                   'accuracy': accuracy,\n",
    "                   'av': av,\n",
    "                   'learning_rate': learning_rate,\n",
    "\n",
    "                   'w_in_val': cell.w_in_val,\n",
    "                   'w_rec_val': cell.w_rec_val,\n",
    "                   'w_out': w_out,\n",
    "                   }\n",
    "if FLAGS.model == 'LSNN':\n",
    "    results_tensors['b_out'] = b_out\n",
    "\n",
    "plot_result_tensors = {'input_spikes': input_pixels,\n",
    "                       'z': z,\n",
    "                       'psp': psp,\n",
    "                       'out_plot': out_plot,\n",
    "                       'Y_predict': Y_predict,\n",
    "                       'z_regular': z_regular,\n",
    "                       'z_adaptive': z_adaptive,\n",
    "                       'targets': targets}\n",
    "if FLAGS.model == 'LSNN':\n",
    "    plot_result_tensors['b_con'] = b\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, epoch 0 validation accuracy 0.0859 \n",
      "\n",
      "            firing rate (Hz)  min 0 (220) \t max 0 (220) \t average 0 +- std 0 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 2.3 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 0s\n",
      "            \n",
      "Iteration 200, epoch 0 validation accuracy 0.258 \n",
      "\n",
      "            firing rate (Hz)  min 0 (91) \t max 684 (1) \t average 254 +- std 296 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.9 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 2s\n",
      "            \n",
      "Iteration 400, epoch 0 validation accuracy 0.359 \n",
      "\n",
      "            firing rate (Hz)  min 0 (88) \t max 694 (1) \t average 248 +- std 301 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.7 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 600, epoch 0 validation accuracy 0.285 \n",
      "\n",
      "            firing rate (Hz)  min 0 (86) \t max 668 (1) \t average 227 +- std 283 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.8 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 800, epoch 0 validation accuracy 0.445 \n",
      "\n",
      "            firing rate (Hz)  min 0 (85) \t max 688 (1) \t average 234 +- std 294 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.5 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 1000, epoch 1 validation accuracy 0.426 \n",
      "\n",
      "            firing rate (Hz)  min 0 (85) \t max 695 (1) \t average 228 +- std 294 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.5 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 2.1s\n",
      "            \n",
      "Iteration 1200, epoch 1 validation accuracy 0.465 \n",
      "\n",
      "            firing rate (Hz)  min 0 (86) \t max 686 (1) \t average 227 +- std 289 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.4 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 1400, epoch 1 validation accuracy 0.492 \n",
      "\n",
      "            firing rate (Hz)  min 0 (85) \t max 691 (1) \t average 231 +- std 293 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.3 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 1600, epoch 1 validation accuracy 0.539 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 686 (1) \t average 228 +- std 290 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.3 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 1800, epoch 2 validation accuracy 0.551 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 692 (1) \t average 230 +- std 291 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.4 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 2000, epoch 2 validation accuracy 0.523 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 682 (1) \t average 226 +- std 286 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.3 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 2s\n",
      "            \n",
      "Iteration 2200, epoch 2 validation accuracy 0.527 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 676 (1) \t average 221 +- std 281 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.2 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 2400, epoch 2 validation accuracy 0.594 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 687 (1) \t average 225 +- std 287 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.01 \t training op. time 1.9s\n",
      "            \n",
      "Decaying learning rate: 0.01 -> 0.008\n",
      "Iteration 2600, epoch 3 validation accuracy 0.566 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 679 (1) \t average 221 +- std 283 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.3 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 2800, epoch 3 validation accuracy 0.59 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 679 (1) \t average 220 +- std 282 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.2 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 3000, epoch 3 validation accuracy 0.551 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 675 (1) \t average 216 +- std 278 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.2 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 3200, epoch 3 validation accuracy 0.59 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 684 (1) \t average 220 +- std 283 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 3400, epoch 3 validation accuracy 0.57 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 678 (1) \t average 218 +- std 280 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 3600, epoch 4 validation accuracy 0.617 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 679 (1) \t average 217 +- std 279 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 3800, epoch 4 validation accuracy 0.605 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 676 (1) \t average 215 +- std 276 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 4000, epoch 4 validation accuracy 0.598 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 675 (1) \t average 213 +- std 274 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 4200, epoch 4 validation accuracy 0.637 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 681 (1) \t average 216 +- std 277 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 4400, epoch 5 validation accuracy 0.57 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 682 (1) \t average 214 +- std 276 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 4600, epoch 5 validation accuracy 0.633 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 679 (1) \t average 213 +- std 275 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 4800, epoch 5 validation accuracy 0.645 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 672 (1) \t average 208 +- std 269 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1 \t regularization loss 0\n",
      "            learning rate 0.008 \t training op. time 1.8s\n",
      "            \n",
      "Decaying learning rate: 0.008 -> 0.0064\n",
      "Iteration 5000, epoch 5 validation accuracy 0.645 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 674 (1) \t average 209 +- std 271 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 2s\n",
      "            \n",
      "Iteration 5200, epoch 6 validation accuracy 0.57 \n",
      "\n",
      "            firing rate (Hz)  min 0 (84) \t max 683 (1) \t average 212 +- std 274 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 5400, epoch 6 validation accuracy 0.59 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 671 (1) \t average 210 +- std 270 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 5600, epoch 6 validation accuracy 0.598 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 671 (1) \t average 212 +- std 270 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1.1 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 5800, epoch 6 validation accuracy 0.633 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 676 (1) \t average 213 +- std 268 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 1 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 6000, epoch 6 validation accuracy 0.66 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 690 (1) \t average 218 +- std 273 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.97 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 6200, epoch 7 validation accuracy 0.688 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 690 (1) \t average 217 +- std 272 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.96 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 6400, epoch 7 validation accuracy 0.621 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 693 (1) \t average 217 +- std 271 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.99 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 6600, epoch 7 validation accuracy 0.734 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 689 (1) \t average 216 +- std 269 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.77 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.9s\n",
      "            \n",
      "Iteration 6800, epoch 7 validation accuracy 0.633 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 682 (1) \t average 213 +- std 267 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.96 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 7000, epoch 8 validation accuracy 0.75 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 682 (1) \t average 212 +- std 266 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.77 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 7200, epoch 8 validation accuracy 0.676 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 688 (1) \t average 210 +- std 264 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.94 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n",
      "Iteration 7400, epoch 8 validation accuracy 0.668 \n",
      "\n",
      "            firing rate (Hz)  min 0 (83) \t max 687 (1) \t average 212 +- std 266 (over neurons)\n",
      "            connectivity (total 0.997)\t W_in 1 \t W_rec 1 \t\t w_out 1\n",
      "            number of non zero weights \t W_in 17600/17600 \t W_rec 48180/48400 \t w_out 2200/2200\n",
      "\n",
      "            classification loss 0.91 \t regularization loss 0\n",
      "            learning rate 0.0064 \t training op. time 1.8s\n",
      "            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32mtest\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     92\u001B[0m     \u001B[0mt0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     93\u001B[0m     \u001B[0mtrain_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_img\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_data_dict\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mFLAGS\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_batch_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'train'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 94\u001B[0;31m     \u001B[0mfinal_state_value\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0m_\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msess\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mfinal_state\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_step\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtrain_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mrun_options\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     95\u001B[0m     \u001B[0mt_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mt0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36mrun\u001B[0;34m(self, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m    927\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    928\u001B[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001B[0;32m--> 929\u001B[0;31m                          run_metadata_ptr)\n\u001B[0m\u001B[1;32m    930\u001B[0m       \u001B[0;32mif\u001B[0m \u001B[0mrun_metadata\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    931\u001B[0m         \u001B[0mproto_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf_session\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mTF_GetBuffer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrun_metadata_ptr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run\u001B[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1150\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mfinal_fetches\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mfinal_targets\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mfeed_dict_tensor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1151\u001B[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001B[0;32m-> 1152\u001B[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001B[0m\u001B[1;32m   1153\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1154\u001B[0m       \u001B[0mresults\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_run\u001B[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001B[0m\n\u001B[1;32m   1326\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mhandle\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1327\u001B[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001B[0;32m-> 1328\u001B[0;31m                            run_metadata)\n\u001B[0m\u001B[1;32m   1329\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1330\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_do_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_prun_fn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeeds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfetches\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_do_call\u001B[0;34m(self, fn, *args)\u001B[0m\n\u001B[1;32m   1332\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_do_call\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1333\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1334\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mfn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1335\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0merrors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mOpError\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1336\u001B[0m       \u001B[0mmessage\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcompat\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mas_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_run_fn\u001B[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001B[0m\n\u001B[1;32m   1317\u001B[0m       \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_extend_graph\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1318\u001B[0m       return self._call_tf_sessionrun(\n\u001B[0;32m-> 1319\u001B[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001B[0m\u001B[1;32m   1320\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1321\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m_prun_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001B[0m in \u001B[0;36m_call_tf_sessionrun\u001B[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001B[0m\n\u001B[1;32m   1405\u001B[0m     return tf_session.TF_SessionRun_wrapper(\n\u001B[1;32m   1406\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_session\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptions\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_list\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1407\u001B[0;31m         run_metadata)\n\u001B[0m\u001B[1;32m   1408\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1409\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0m_call_tf_sessionprun\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhandle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfeed_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfetch_list\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "t_train = 0\n",
    "for k_iter in range(FLAGS.n_iter):\n",
    "\n",
    "    # Decaying learning rate\n",
    "    if k_iter > 0 and np.mod(k_iter, FLAGS.lr_decay_every) == 0 and mnist.train._epochs_completed > 0:\n",
    "        old_lr = sess.run(learning_rate)\n",
    "        new_lr = sess.run(decay_learning_rate_op)\n",
    "        print('Decaying learning rate: {:.2g} -> {:.2g}'.format(old_lr, new_lr))\n",
    "\n",
    "    # Print some values to monitor convergence\n",
    "    if np.mod(k_iter, FLAGS.print_every) == 0:\n",
    "\n",
    "        val_dict, input_img = get_data_dict(FLAGS.n_batch_validation, type='validation')\n",
    "        results_values, plot_results_values = sess.run([results_tensors, plot_result_tensors], feed_dict=val_dict)\n",
    "\n",
    "        if FLAGS.save_data:\n",
    "            save_file(results_values, storage_path, 'results_values', 'pickle')\n",
    "            save_file(plot_results_values, storage_path, 'plot_results_values', 'pickle')\n",
    "\n",
    "        # Storage of the results\n",
    "        test_loss_with_reg_list.append(results_values['loss_reg'])\n",
    "        test_loss_list.append(results_values['loss_recall'])\n",
    "        test_error_list.append(results_values['accuracy'])\n",
    "        training_time_list.append(t_train)\n",
    "\n",
    "        print(\n",
    "            '''Iteration {}, epoch {} validation accuracy {:.3g} '''\n",
    "                .format(k_iter, mnist.train._epochs_completed, test_error_list[-1],))\n",
    "\n",
    "\n",
    "        def get_stats(v):\n",
    "            if np.size(v) == 0:\n",
    "                return np.nan, np.nan, np.nan, np.nan\n",
    "            min_val = np.min(v)\n",
    "            max_val = np.max(v)\n",
    "\n",
    "            k_min = np.sum(v == min_val)\n",
    "            k_max = np.sum(v == max_val)\n",
    "\n",
    "            return np.min(v), np.max(v), np.mean(v), np.std(v), k_min, k_max\n",
    "\n",
    "\n",
    "        firing_rate_stats = get_stats(results_values['av'] * 1000)\n",
    "\n",
    "        # some connectivity statistics\n",
    "        rewired_ref_list = ['w_in_val', 'w_rec_val', 'w_out']\n",
    "        non_zeros = [np.sum(results_values[ref] != 0) for ref in rewired_ref_list]\n",
    "        sizes = [np.size(results_values[ref]) for ref in rewired_ref_list]\n",
    "        empirical_connectivity = np.sum(non_zeros) / np.sum(sizes)\n",
    "        empirical_connectivities = [nz / size for nz, size in zip(non_zeros, sizes)]\n",
    "\n",
    "        if FLAGS.verbose:\n",
    "            print('''\n",
    "            firing rate (Hz)  min {:.0f} ({}) \\t max {:.0f} ({}) \\t average {:.0f} +- std {:.0f} (over neurons)\n",
    "            connectivity (total {:.3g})\\t W_in {:.3g} \\t W_rec {:.2g} \\t\\t w_out {:.2g}\n",
    "            number of non zero weights \\t W_in {}/{} \\t W_rec {}/{} \\t w_out {}/{}\n",
    "\n",
    "            classification loss {:.2g} \\t regularization loss {:.2g}\n",
    "            learning rate {:.2g} \\t training op. time {:.2g}s\n",
    "            '''.format(\n",
    "                firing_rate_stats[0], firing_rate_stats[4], firing_rate_stats[1], firing_rate_stats[5],\n",
    "                firing_rate_stats[2], firing_rate_stats[3],\n",
    "                empirical_connectivity,\n",
    "                empirical_connectivities[0], empirical_connectivities[1], empirical_connectivities[2],\n",
    "                non_zeros[0], sizes[0],\n",
    "                non_zeros[1], sizes[1],\n",
    "                non_zeros[2], sizes[2],\n",
    "                results_values['loss_recall'], results_values['loss_reg'],\n",
    "                results_values['learning_rate'], t_train,\n",
    "            ))\n",
    "\n",
    "        # Save files result\n",
    "        if FLAGS.save_data:\n",
    "            results = {\n",
    "                'error': test_error_list[-1],\n",
    "                'loss': test_loss_list[-1],\n",
    "                'loss_with_reg': test_loss_with_reg_list[-1],\n",
    "                'loss_with_reg_list': test_loss_with_reg_list,\n",
    "                'error_list': test_error_list,\n",
    "                'loss_list': test_loss_list,\n",
    "                'time_to_ref': time_to_ref_list,\n",
    "                'training_time': training_time_list,\n",
    "                'tau_delay_list': tau_delay_list,\n",
    "                'flags': flag_dict,\n",
    "            }\n",
    "            save_file(results, storage_path, 'results', file_type='json')\n",
    "\n",
    "        if FLAGS.interactive_plot:\n",
    "            update_mnist_plot(ax_list, fig, plt, cell, FLAGS, plot_results_values)\n",
    "\n",
    "    # train\n",
    "    t0 = time()\n",
    "    train_dict, input_img = get_data_dict(FLAGS.n_batch_train, type='train')\n",
    "    final_state_value, _ = sess.run([final_state, train_step], feed_dict=train_dict, options=run_options)\n",
    "    t_train = time() - t0\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if FLAGS.interactive_plot:\n",
    "    update_mnist_plot(ax_list, fig, plt, cell, FLAGS, plot_results_values)\n",
    "\n",
    "\n",
    "if FLAGS.save_data:\n",
    "    # Save the tensorflow graph\n",
    "    saver.save(sess, os.path.join(storage_path, 'session'))\n",
    "    saver.export_meta_graph(os.path.join(storage_path, 'graph.meta'))\n",
    "    print(\"Network meta graph and session saved. Now testing...\")\n",
    "\n",
    "    # Testing\n",
    "    test_errors = []\n",
    "    n_test_batches = (mnist.test.num_examples//FLAGS.n_batch_validation) + 1\n",
    "    for i in range(n_test_batches):  # cover the whole test set\n",
    "        test_dict, input_img = get_data_dict(FLAGS.n_batch_validation, type='test')\n",
    "\n",
    "        results_values, plot_results_values, in_spk, spk, targets_np = sess.run(\n",
    "            [results_tensors, plot_result_tensors, input_pixels, z, targets],\n",
    "            feed_dict=test_dict)\n",
    "        test_errors.append(results_values['accuracy'])\n",
    "\n",
    "    print('''Statistics on the test set: average accuracy {:.3g} +- {:.3g} (averaged over {} test batches of size {})'''\n",
    "          .format(np.mean(test_errors), np.std(test_errors), n_test_batches, FLAGS.n_batch_validation))\n",
    "    plot_results_values['test_imgs'] = np.array(input_img)\n",
    "    save_file(plot_results_values, storage_path, 'plot_results_values', 'pickle')\n",
    "    save_file(results_values, storage_path, 'results_values', 'pickle')\n",
    "\n",
    "    # Save files result\n",
    "    results = {\n",
    "        'test_errors': test_errors,\n",
    "        'test_errors_mean': np.mean(test_errors),\n",
    "        'test_errors_std': np.std(test_errors),\n",
    "        'error': test_error_list[-1] if test_error_list else None,\n",
    "        'loss': test_loss_list[-1] if test_loss_list else None,\n",
    "        'loss_with_reg': test_loss_with_reg_list[-1],\n",
    "        'loss_with_reg_list': test_loss_with_reg_list,\n",
    "        'error_list': test_error_list,\n",
    "        'loss_list': test_loss_list,\n",
    "        'time_to_ref': time_to_ref_list,\n",
    "        'training_time': training_time_list,\n",
    "        'tau_delay_list': tau_delay_list,\n",
    "        'flags': flag_dict,\n",
    "    }\n",
    "\n",
    "    save_file(results, storage_path, 'results', file_type='json')\n",
    "\n",
    "    if FLAGS.interactive_plot:\n",
    "        for i in range(min(8, FLAGS.n_batch_validation)):\n",
    "            update_mnist_plot(ax_list, fig, plt, cell, FLAGS, plot_results_values, batch=i)\n",
    "            fig.savefig(os.path.join(storage_path, 'figure_TEST_' + str(i) + '.pdf'), format='pdf')\n",
    "            plt.show()\n",
    "            plt.ioff()\n",
    "del sess\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_data_dict(1).__iter__().__next__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trans, img = get_data_dict(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pickle.dump((trans.values().__iter__().__next__()\n",
    ", img), open('some_img', 'wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trans.values().__iter__().__next__()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cell.state_size"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}